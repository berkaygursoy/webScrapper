# Headline Scraper

Modern web scraping tool that extracts headlines from HaberTÃ¼rk's homepage and exports them to an Excel file.

## Features

- âœ… Dynamic HTML parsing with BeautifulSoup
- âœ… Error handling for network issues and website structure changes
- âœ… Clear output formatting in console and Excel
- âœ… Uses proper HTTP headers to mimic legitimate browser traffic
- âœ… Tracks source and timestamps data
- ğŸ“Š Well-formatted Excel export compatible with modern spreadsheet applications

## Installation & Requirements

To use this script, you'll need:

1. Python 3.7+
2. Required libraries:
   - requests (`pip install requests`)
   - beautifulsoup4 (`pip install beautifulsoup4`)
   - pandas (`pip install pandas`)

Install all dependencies with:
```bash
pip install requests beautifulsoup4 pandas openpyxl


## Usage

Save the code in a file named haberturk_scraper.py
Run the script from command line:
python haberturk_scraper.py
The scraped headlines will be saved to an Excel file named haberturk_headlines.xlsx](https://img.shields.io/badge/Python-3.8%252B-blue
https://img.shields.io/badge/License-MIT-green
https://img.shields.io/badge/Status-Active-brightgreen

Bu proje, web sayfalarÄ±ndan veri Ã§ekip Excel formatÄ±nda kaydeden Python tabanlÄ± bir uygulamadÄ±r.

ğŸ“‹ Proje AÃ§Ä±klamasÄ±
Python'un requests, BeautifulSoup ve pandas kÃ¼tÃ¼phanelerini kullanarak web sayfalarÄ±ndan veri Ã§eken ve bu verileri Excel formatÄ±nda kaydeden bir script.

ğŸš€ Ã–zellikler
ğŸŒ Web sayfalarÄ±ndan HTTP istekleri ile iÃ§erik Ã§ekme

ğŸ§¹ BeautifulSoup ile HTML iÃ§eriÄŸini ayrÄ±ÅŸtÄ±rma

ğŸ“Š Pandas ile veri iÅŸleme ve dÃ¼zenleme

ğŸ’¾ Excel formatÄ±nda veri kaydetme

âš¡ HÄ±zlÄ± ve etkili veri iÅŸleme

ğŸ“¦ Kurulum

Gereksinimler
Python 3.8+
pip (Python paket yÃ¶neticisi)

ğŸ› ï¸ KullanÄ±m

Temel KullanÄ±m

python scraper.py
Ã–zelleÅŸtirme
Kodu farklÄ± web siteleri iÃ§in Ã¶zelleÅŸtirmek iÃ§in aÅŸaÄŸÄ±daki deÄŸiÅŸkenleri dÃ¼zenleyebilirsiniz:

python
# URL'yi deÄŸiÅŸtirin
url = "https://hedef-site.com/"

# FarklÄ± HTML elementleri kullanÄ±n
basliks = soup.find_all("h1")  # veya "h2", "h3", "div", etc.

# FarklÄ± Ã§Ä±ktÄ± formatlarÄ±
df.to_csv("sonuclar.csv", index=False)  # CSV formatÄ±nda kaydetme

ğŸ“ Dosya YapÄ±sÄ±

web-scraping-project/
â”‚
â”œâ”€â”€ scraper.py                 # Ana scraping scripti
â”œâ”€â”€ requirements.txt           # BaÄŸÄ±mlÄ±lÄ±klar
â”œâ”€â”€ example_basliklar.xlsx     # Ã–rnek Ã§Ä±ktÄ± dosyasÄ±
â”œâ”€â”€ README.md                  # Bu dosya
â””â”€â”€ .gitignore                 # Git ignore dosyasÄ±

ğŸ“ Ã–rnek Ã‡Ä±ktÄ±

Ã‡alÄ±ÅŸtÄ±rma sonrasÄ±nda aÅŸaÄŸÄ±daki gibi bir Ã§Ä±ktÄ± alacaksÄ±nÄ±z:
Haber baÅŸlÄ±klarÄ± 'example_basliklar.xlsx' dosyasÄ±na kaydedildi.
OluÅŸturulan Excel dosyasÄ± ÅŸu ÅŸekilde gÃ¶rÃ¼necektir:

BaÅŸlÄ±klar

Ä°lk haber baÅŸlÄ±ÄŸÄ±
Ä°kinci haber baÅŸlÄ±ÄŸÄ±
ÃœÃ§Ã¼ncÃ¼ haber baÅŸlÄ±ÄŸÄ±
ğŸ”§ GeliÅŸtirme

BaÄŸÄ±mlÄ±lÄ±klar

Proje ÅŸu kÃ¼tÃ¼phanelere baÄŸlÄ±dÄ±r:

requests - HTTP istekleri iÃ§in

beautifulsoup4 - HTML ayrÄ±ÅŸtÄ±rma iÃ§in

pandas - Veri iÅŸleme iÃ§in

openpyxl - Excel Ã§Ä±ktÄ±larÄ± iÃ§in
